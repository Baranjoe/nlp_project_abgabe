{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Pfad zur Datei\n",
    "tsv_path = \"filtered_transcripted_train_all.tsv\"\n",
    "df = pd.read_csv(tsv_path, sep=\"\\t\", index_col=0)\n",
    "\n",
    "print(df.head())\n",
    "print(df.columns)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ca52aad0cda0f8c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tot = len(df)\n",
    "uniq = df['sentence'].nunique()\n",
    "\n",
    "print(\"Total Einträge in DataFrame:\", tot)\n",
    "print(\"Einzigartige Einträge in DataFrame:\", uniq)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4fbb1e3e88bbae48",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "audio_len = df.duration.mean()\n",
    "word_sent = df['sentence_norm'].str.split().str.len().mean()\n",
    "\n",
    "print('Durchschnittliche Audio Länge [s]:', audio_len)\n",
    "print('Durchschnittliche Anzahl Wörter pro Satz:', word_sent)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8c1a24f250c9f2a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Verteilung der Confidence-Werte\n",
    "Die folgenden Plots zeigen die Verteilung der Confidence-Scores und markieren Ausreißer für eine spätere Filterung.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5c4e5d415ba8b5d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159ea9ebcfb87555",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.hist(\"confidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Verteilung der WER-Werte\n",
    "Zusätzlich zum Confidence-Blick wird die WER-Verteilung visualisiert, um problematische Segmente schneller zu identifizieren.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2be90647d875d38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bf27684350536f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.hist(\"WER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed43e481026982a7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Embeddings berechnen und Kennzahlen hinzufügen\n",
    "Embeddings, Similarity-Scores und zusätzliche Spalten werden hier erzeugt und persistiert, sodass jeder weitere Schritt auf denselben Merkmalen aufsetzt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87df6ca0623a6114",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1. TSV einlesen\n",
    "# ---------------------------------------------------\n",
    "tsv_path = \"filtered_transcripted_train_all.tsv\"   # <--- anpassen\n",
    "df = pd.read_csv(tsv_path, sep=\"\\t\", index_col=0)\n",
    "\n",
    "# Wir verwenden:\n",
    "# - clip_path\n",
    "# - sentence_norm\n",
    "# - transcript_norm\n",
    "# - WER   (ist schon im TSV vorhanden)\n",
    "\n",
    "print(df[[\"clip_path\", \"sentence_norm\", \"transcript_norm\", \"WER\"]].head())\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2. Sentence-Transformer laden\n",
    "# ---------------------------------------------------\n",
    "model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3. Embeddings für alle Sätze berechnen (batchweise)\n",
    "# ---------------------------------------------------\n",
    "sentences_ref = df[\"sentence_norm\"].tolist()\n",
    "sentences_hyp = df[\"transcript_norm\"].tolist()\n",
    "\n",
    "# normalize_embeddings=True -> Cosine Similarity = einfacher Dot-Product\n",
    "ref_embs = model.encode(\n",
    "    sentences_ref,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "hyp_embs = model.encode(\n",
    "    sentences_hyp,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"ref_embs shape:\", ref_embs.shape)  # (N, dim)\n",
    "print(\"hyp_embs shape:\", hyp_embs.shape)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4. Cosine Similarity + Quality Score (WER aus TSV)\n",
    "# ---------------------------------------------------\n",
    "# Cosine sim = Zeilenweises Dot-Produkt\n",
    "emb_sim = np.sum(ref_embs * hyp_embs, axis=1)\n",
    "\n",
    "df[\"emb_sim\"] = emb_sim\n",
    "\n",
    "alpha = 0.6  # Gewichtung Embedding vs. WER\n",
    "\n",
    "# WER ist schon in [0,1]; wir nehmen (1 - WER) als \"Word-Accuracy\"\n",
    "df[\"quality_score\"] = alpha * df[\"emb_sim\"] + (1 - alpha) * (1.0 - df[\"WER\"].astype(float))\n",
    "\n",
    "# Optional: kurz anschauen\n",
    "print(df[[\n",
    "    \"sentence_norm\",\n",
    "    \"transcript_norm\",\n",
    "    \"WER\",\n",
    "    \"emb_sim\",\n",
    "    \"quality_score\"\n",
    "]].head())\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5. Embeddings separat als .npz speichern\n",
    "# ---------------------------------------------------\n",
    "ids = df[\"clip_path\"].astype(str).values   # eindeutige ID pro Sample\n",
    "row_idx = df.index.values                  # ursprünglicher DataFrame-Index\n",
    "\n",
    "npz_path = \"whisper_embeddings_ref_hyp_paraphrase-multilingual-MiniLM-L12-v2.npz\"\n",
    "\n",
    "np.savez(\n",
    "    npz_path,\n",
    "    ids=ids,       # z.B. \"0f19ca1d-.../....flac\"\n",
    "    row_idx=row_idx,\n",
    "    ref=ref_embs,  # Shape (N, dim)\n",
    "    hyp=hyp_embs   # Shape (N, dim)\n",
    ")\n",
    "\n",
    "print(f\"Embeddings gespeichert in: {npz_path}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 6. TSV mit neuen Spalten speichern\n",
    "# ---------------------------------------------------\n",
    "out_tsv_path = \"your_file_with_embscores.tsv\"\n",
    "df.to_csv(out_tsv_path, sep=\"\\t\")\n",
    "print(f\"TSV mit emb_sim und quality_score gespeichert in: {out_tsv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43b83fc2380aa37",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## \"Hall of Fame\" und \"Hall of Shame\" der Transkripte\n",
    "Dieser Abschnitt stellt Beispiele mit besonders guten beziehungsweise schlechten Transkriptionen zusammen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9547290b12347f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Wenn du den DF schon im Speicher hast, kannst du das Einlesen skippen\n",
    "tsv_path = \"your_file_with_embscores.tsv\"   # anpassen\n",
    "df = pd.read_csv(tsv_path, sep=\"\\t\", index_col=0)\n",
    "\n",
    "# Nur zur Sicherheit:\n",
    "assert \"emb_sim\" in df.columns, \"Spalte 'emb_sim' nicht gefunden!\"\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 10 beste (höchste Embedding-Similarity)\n",
    "# ---------------------------------------------------\n",
    "best_10 = df.sort_values(\"emb_sim\", ascending=False).head(10).copy()\n",
    "\n",
    "print(\"===== 10 BESTEN NUR NACH EMBEDDING-SIMILARITY =====\")\n",
    "print(best_10[[\n",
    "    \"clip_path\",\n",
    "    \"sentence_norm\",\n",
    "    \"transcript_norm\",\n",
    "    \"emb_sim\",\n",
    "    \"WER\"          # falls du WER daneben sehen willst\n",
    "]].to_string(index=False))\n",
    "\n",
    "# Optional: als TSV speichern\n",
    "best_10.to_csv(\"top10_embedding.tsv\", sep=\"\\t\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 10 schlechtesten (niedrigste Embedding-Similarity)\n",
    "# ---------------------------------------------------\n",
    "worst_10 = df.sort_values(\"emb_sim\", ascending=True).head(10).copy()\n",
    "\n",
    "print(\"\\n===== 10 SCHLECHTESTEN NUR NACH EMBEDDING-SIMILARITY =====\")\n",
    "print(worst_10[[\n",
    "    \"clip_path\",\n",
    "    \"sentence_norm\",\n",
    "    \"transcript_norm\",\n",
    "    \"emb_sim\",\n",
    "    \"WER\"\n",
    "]].to_string(index=False))\n",
    "\n",
    "worst_10.to_csv(\"bottom10_embedding.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Weitere mögliche Auswertungen\n",
    "- Verhältnis männlich/weiblich nach Kanton\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "663c97237c500525"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Verteilung der Embedding-Similarity\n",
    "Die Embedding-Scores werden nach denselben Kriterien ausgewertet wie Confidence und WER.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d570d53e30612f23"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.hist(\"emb_sim\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad492c15902d2d68",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Beispiele nach Embedding-Score-Bereich\n",
    "Zur qualitativen Analyse können Beispiele innerhalb definierter Similarity-Intervalle ausgegeben werden.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8262e7564ee9c116"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Filter nach emb_sim zwischen 0.70 und 0.75\n",
    "subset = df[(df[\"emb_sim\"] >= 0.60) & (df[\"emb_sim\"] <= 0.65)]\n",
    "\n",
    "# 10 zufällige Zeilen ziehen (oder weniger, falls nicht genug vorhanden)\n",
    "examples = subset.sample(n=min(10, len(subset)), random_state=42)\n",
    "\n",
    "# Nur gewünschte Spalten anzeigen\n",
    "examples[[\"WER\", \"emb_sim\", \"sentence_norm\", \"transcript_norm\"]]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39477a8e9ffda311",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Historische Version (veraltet)\n",
    "Die weiterhin gespeicherten Zellen dokumentieren die frühere Umsetzung und dienen lediglich als Referenz.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dee5e71834f3f663"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Filter anwenden\n",
    "mask = (\n",
    "    (df[\"emb_sim\"] >= 0.8) &     # emb_sim >= 0.7\n",
    "    (df[\"WER\"] <= 0.5) &         # WER <= 0.8\n",
    "    (df[\"canton\"] != \"Wallis\")   # Canton != Wallis\n",
    ")\n",
    "\n",
    "df_clean = df[mask].copy().reset_index(drop=True)\n",
    "\n",
    "# 2) Basis-Stats\n",
    "n_before = len(df)\n",
    "n_after = len(df_clean)\n",
    "n_removed = n_before - n_after\n",
    "\n",
    "# 3) Anteil WER = 0 in gefiltertem Set\n",
    "n_wer_zero = (df_clean[\"WER\"] == 0).sum()\n",
    "share_wer_zero = n_wer_zero / n_after if n_after > 0 else 0.0\n",
    "\n",
    "print(f\"Gesamt vorher: {n_before}\")\n",
    "print(f\"Übrig nach Filter: {n_after}\")\n",
    "print(f\"Entfernt: {n_removed} ({n_removed / n_before:.1%} der Daten)\")\n",
    "\n",
    "print(f\"\\nDavon WER = 0: {n_wer_zero} Beispiele\")\n",
    "print(f\"Anteil WER = 0 im gefilterten Set: {share_wer_zero:.1%}\")\n",
    "\n",
    "# Optional: als neue Trainingsbasis speichern\n",
    "df_clean.to_csv(\"emb_scores_clean_filtered.csv\", index=False)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a90f30eef7794d7",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Erstellung der Train-/Test-Datensätze (aktuelle Version)\n",
    "Dieser Abschnitt erstellt die finalen Splits für Training und Test und knüpft an die oben erzeugten Merkmalslisten an.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b35730658052127"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Datenaufbereitung für ASR-Post-Editing-Experimente.\n",
    "\n",
    "- Liest separate TSV-Dateien für Train und Test ein\n",
    "- Wendet drei Varianten an:\n",
    "    1) \"all\":     kein Filter (nur Wallis wird ausgeschlossen)\n",
    "    2) \"relaxed\": emb_sim >= 0.75 (falls vorhanden), WER <= 0.7, canton != \"Wallis\"\n",
    "    3) \"strict\":  emb_sim >= 0.8 (falls vorhanden),  WER <= 0.5, canton != \"Wallis\"\n",
    "- Für jede Variante:\n",
    "    * Basis-Stats für Train/Test nach Filterung ausgeben\n",
    "    * Gefilterte Train/Test/All-CSV schreiben (kein erneuter Split)\n",
    "    * Train-Set mit harten Beispielen oversamplen und zusätzliche CSV speichern\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Konfiguration\n",
    "# -------------------------------------------------------\n",
    "\n",
    "TRAIN_TSV = Path(\"filtered_transcripted_train_all.tsv\")\n",
    "TEST_TSV = Path(\"filtered_transcripted_test.tsv\")\n",
    "OUTPUT_DIR = Path(\"data_prepared\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "OVERSAMPLE_FACTOR = 2\n",
    "\n",
    "REQUIRED_COLUMNS = {\"sentence_norm\", \"transcript_norm\", \"WER\", \"canton\"}\n",
    "\n",
    "CONFIGS: list[dict[str, object]] = [\n",
    "    {\n",
    "        \"name\": \"all\",\n",
    "        \"emb_min\": None,\n",
    "        \"wer_max\": None,\n",
    "        \"exclude_wallis\": True,\n",
    "        \"hard_wer_low\": 0.4,\n",
    "        \"hard_wer_high\": None,  # dynamisch anhand der Daten\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"relaxed\",\n",
    "        \"emb_min\": 0.75,\n",
    "        \"wer_max\": 0.7,\n",
    "        \"exclude_wallis\": True,\n",
    "        \"hard_wer_low\": 0.4,\n",
    "        \"hard_wer_high\": 0.7,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"strict\",\n",
    "        \"emb_min\": 0.8,\n",
    "        \"wer_max\": 0.5,\n",
    "        \"exclude_wallis\": True,\n",
    "        \"hard_wer_low\": 0.3,\n",
    "        \"hard_wer_high\": 0.5,\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Hilfsfunktionen\n",
    "# -------------------------------------------------------\n",
    "\n",
    "def print_basic_stats(df: pd.DataFrame, label: str) -> None:\n",
    "    \"\"\"Ein paar Basis-Statistiken für einen gefilterten DataFrame ausgeben.\"\"\"\n",
    "    n = len(df)\n",
    "    if n == 0:\n",
    "        print(f\"\\n=== Basis-Stats: {label} ===\")\n",
    "        print(\"Keine Beispiele nach Filterung vorhanden.\")\n",
    "        return\n",
    "\n",
    "    n_wer_zero = (df[\"WER\"] == 0).sum()\n",
    "    share_wer_zero = n_wer_zero / n if n > 0 else 0.0\n",
    "\n",
    "    print(f\"\\n=== Basis-Stats: {label} ===\")\n",
    "    print(f\"Anzahl Beispiele: {n}\")\n",
    "    print(f\"Davon WER = 0: {n_wer_zero} ({share_wer_zero:.1%})\")\n",
    "    print(f\"WER-Mean: {df['WER'].mean():.4f}\")\n",
    "    if \"emb_sim\" in df.columns:\n",
    "        print(f\"emb_sim-Mean: {df['emb_sim'].mean():.4f}\")\n",
    "\n",
    "\n",
    "def ensure_required_columns(df: pd.DataFrame, split_name: str) -> None:\n",
    "    missing = REQUIRED_COLUMNS.difference(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Fehlende Spalten in {split_name}-Split: {missing}\")\n",
    "\n",
    "\n",
    "def load_split(path: Path, split_name: str) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Datei nicht gefunden: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=\"\\t\", index_col=0)\n",
    "    df = df.reset_index(drop=True)\n",
    "    ensure_required_columns(df, split_name)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_filter(df: pd.DataFrame, cfg: dict[str, object], split_name: str) -> pd.DataFrame:\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    df_clean[\"WER\"] = pd.to_numeric(df_clean[\"WER\"], errors=\"coerce\")\n",
    "    if \"emb_sim\" in df_clean.columns:\n",
    "        df_clean[\"emb_sim\"] = pd.to_numeric(df_clean[\"emb_sim\"], errors=\"coerce\")\n",
    "\n",
    "    mask = pd.Series(True, index=df_clean.index)\n",
    "\n",
    "    emb_min = cfg.get(\"emb_min\")\n",
    "    if emb_min is not None:\n",
    "        if \"emb_sim\" not in df_clean.columns:\n",
    "            print(\n",
    "                f\"WARNUNG: Spalte 'emb_sim' nicht vorhanden ({split_name}). \"\n",
    "                \"emb_min-Filter wird übersprungen.\"\n",
    "            )\n",
    "        else:\n",
    "            mask &= df_clean[\"emb_sim\"] >= float(emb_min)\n",
    "\n",
    "    wer_max = cfg.get(\"wer_max\")\n",
    "    if wer_max is not None:\n",
    "        mask &= df_clean[\"WER\"] <= float(wer_max)\n",
    "\n",
    "    if cfg.get(\"exclude_wallis\", True):\n",
    "        mask &= df_clean[\"canton\"] != \"Wallis\"\n",
    "\n",
    "    df_filtered = df_clean[mask].copy().reset_index(drop=True)\n",
    "\n",
    "    n_before = len(df_clean)\n",
    "    n_after = len(df_filtered)\n",
    "    removed = n_before - n_after\n",
    "    removed_share = (removed / n_before) if n_before else 0.0\n",
    "\n",
    "    print(f\"\\n===== Filter '{cfg['name']}' ({split_name}) =====\")\n",
    "    print(f\"Gesamt vorher: {n_before}\")\n",
    "    print(f\"Übrig nach Filter: {n_after}\")\n",
    "    print(f\"Entfernt: {removed} ({removed_share:.1%} der Daten)\")\n",
    "\n",
    "    print_basic_stats(df_filtered, f\"{cfg['name']} ({split_name})\")\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def save_filtered_sets(cfg_name: str, train_df: pd.DataFrame, test_df: pd.DataFrame) -> None:\n",
    "    cfg_dir = OUTPUT_DIR / cfg_name\n",
    "    cfg_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "    all_path = cfg_dir / f\"emb_scores_clean_{cfg_name}_all.csv\"\n",
    "    train_path = cfg_dir / f\"emb_scores_clean_{cfg_name}_train.csv\"\n",
    "    test_path = cfg_dir / f\"emb_scores_clean_{cfg_name}_test.csv\"\n",
    "\n",
    "    all_df.to_csv(all_path, index=False)\n",
    "    train_df.to_csv(train_path, index=False)\n",
    "    test_df.to_csv(test_path, index=False)\n",
    "\n",
    "    print(f\"Gesamtes Set gespeichert: {all_path}\")\n",
    "    print(f\"Train-Set gespeichert:   {train_path}\")\n",
    "    print(f\"Test-Set gespeichert:    {test_path}\")\n",
    "\n",
    "\n",
    "def oversample_hard_cases(\n",
    "    train_df: pd.DataFrame,\n",
    "    config_name: str,\n",
    "    wer_low: float,\n",
    "    wer_high: float,\n",
    ") -> pd.DataFrame:\n",
    "    cfg_dir = OUTPUT_DIR / config_name\n",
    "    cfg_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if len(train_df) == 0:\n",
    "        print(f\"\\n--- Oversampling '{config_name}' ---\")\n",
    "        print(\"WARNUNG: Kein Train-Set vorhanden – Überspringe Oversampling.\")\n",
    "        oversampled_train = train_df.copy()\n",
    "    else:\n",
    "        wer_high = max(wer_low, wer_high)\n",
    "        hard_mask = (train_df[\"WER\"] >= wer_low) & (train_df[\"WER\"] <= wer_high)\n",
    "        hard_df = train_df[hard_mask]\n",
    "\n",
    "        print(f\"\\n--- Oversampling '{config_name}' ---\")\n",
    "        print(f\"Harte Beispiele (WER in [{wer_low}, {wer_high}]): {len(hard_df)}\")\n",
    "\n",
    "        if len(hard_df) == 0:\n",
    "            print(\"WARNUNG: Keine harten Beispiele gefunden – Oversampling wird übersprungen.\")\n",
    "            oversampled_train = train_df.copy()\n",
    "        else:\n",
    "            hard_oversampled = hard_df.sample(\n",
    "                n=len(hard_df) * (OVERSAMPLE_FACTOR - 1),\n",
    "                replace=True,\n",
    "                random_state=RANDOM_STATE,\n",
    "            )\n",
    "            oversampled_train = (\n",
    "                pd.concat([train_df, hard_oversampled], axis=0)\n",
    "                .sample(frac=1.0, random_state=RANDOM_STATE)\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "            print(f\"Train-Samples vorher: {len(train_df)}\")\n",
    "            print(f\"Train-Samples nach Oversampling: {len(oversampled_train)}\")\n",
    "            print_basic_stats(oversampled_train, f\"Train nach Oversampling '{config_name}'\")\n",
    "\n",
    "    os_path = cfg_dir / f\"emb_scores_clean_{config_name}_train_oversampled.csv\"\n",
    "    oversampled_train.to_csv(os_path, index=False)\n",
    "    print(f\"Oversampled-Train-Set: {os_path}\")\n",
    "\n",
    "    return oversampled_train\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Main\n",
    "# -------------------------------------------------------\n",
    "\n",
    "def main() -> None:\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_df = load_split(TRAIN_TSV, \"train\")\n",
    "    test_df = load_split(TEST_TSV, \"test\")\n",
    "\n",
    "    print(f\"Input Train geladen: {TRAIN_TSV} mit {len(train_df)} Zeilen\")\n",
    "    print(f\"Input Test geladen:  {TEST_TSV} mit {len(test_df)} Zeilen\")\n",
    "\n",
    "    for cfg in CONFIGS:\n",
    "        cfg_name = str(cfg[\"name\"])\n",
    "        print(f\"\\n==============================\\nKonfiguration: {cfg_name}\\n==============================\")\n",
    "\n",
    "        train_filtered = apply_filter(train_df, cfg, \"train\")\n",
    "        test_filtered = apply_filter(test_df, cfg, \"test\")\n",
    "\n",
    "        save_filtered_sets(cfg_name, train_filtered, test_filtered)\n",
    "\n",
    "        wer_low = float(cfg.get(\"hard_wer_low\", 0.4))\n",
    "        wer_high_cfg = cfg.get(\"hard_wer_high\")\n",
    "        if wer_high_cfg is not None:\n",
    "            wer_high = float(wer_high_cfg)\n",
    "        else:\n",
    "            if len(train_filtered):\n",
    "                wer_high = float(train_filtered[\"WER\"].max())\n",
    "            else:\n",
    "                wer_high = wer_low\n",
    "\n",
    "        oversample_hard_cases(train_filtered, cfg_name, wer_low, wer_high)\n",
    "\n",
    "    print(\"\\n=== Fertig. Alle CSVs wurden in\", OUTPUT_DIR, \"geschrieben. ===\")\n",
    "\n",
    "\n",
    "main()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6768f208d6e2098a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5c828380585ef6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tsv_path = \"your_file_with_embscores.tsv\"   # anpassen\n",
    "df = pd.read_csv(tsv_path, sep=\"\\t\", index_col=0)\n",
    "\n",
    "# Falls noch nicht vorhanden: kombinierten Score berechnen\n",
    "alpha = 0.6  # Gewicht Embedding\n",
    "if \"quality_score\" not in df.columns:\n",
    "    df[\"quality_score\"] = alpha * df[\"emb_sim\"] + (1 - alpha) * (1 - df[\"WER\"])\n",
    "\n",
    "# ---------- Variante 1: fixe Schwellwerte ----------\n",
    "# \"hoch\" WER: z.B. >= 0.3\n",
    "# \"hoch\" emb_sim: z.B. >= 0.9\n",
    "mask = (df[\"WER\"] >= 0.30) & (df[\"emb_sim\"] >= 0.90)\n",
    "\n",
    "candidates = df[mask].copy()\n",
    "\n",
    "# zur Sicherheit nach WER absteigend sortieren, dann nach emb_sim absteigend\n",
    "candidates = candidates.sort_values([\"WER\", \"emb_sim\"], ascending=[False, False])\n",
    "\n",
    "top10 = candidates.head(10)\n",
    "\n",
    "print(\"=== 10 Beispiele mit HOHER WER und HOHER emb_sim ===\")\n",
    "print(top10[[\n",
    "    \"clip_path\",\n",
    "    \"sentence_norm\",\n",
    "    \"transcript_norm\",\n",
    "    \"WER\",\n",
    "    \"emb_sim\",\n",
    "    \"quality_score\"\n",
    "]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750a1c2ae6d45ac",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_quick = df[[\"sentence_norm\", \"transcript_norm\"]]\n",
    "df_quick.to_csv(\"ref.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
