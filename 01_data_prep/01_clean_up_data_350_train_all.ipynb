{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufbereitung des Transcripted-Train-Datensatzes\n",
    "Dieses Notebook bündelt die Skripte zur Bereinigung der Trainingsdaten, dokumentiert sämtliche Filterlisten und stellt Hilfsfunktionen für wiederholbare Analysen bereit. Alle Schritte sind so beschrieben, dass sie ohne zusätzlichen Kontext nachvollziehbar bleiben.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook-Überblick\n",
    "- Importiert Statistik- und Audio-Helfer zur Untersuchung der Rohdaten.\n",
    "- Definiert nachvollziehbare Qualitätslisten für problematische Nutzer, Transkripte und Sonderfälle.\n",
    "- Lädt die TSV-Dateien, berechnet Kennzahlen und exportiert bereinigte Varianten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71202a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Distanzmetriken & Textnormalisierung ===\n",
    "import werpy  # Normalisiert Text, berechnet aber kein CER\n",
    "from jiwer import cer, wer  # Liefert CER/WER auf normalisiertem Text\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "smooth = SmoothingFunction().method1\n",
    "\n",
    "# === Allgemeine Bibliotheken ===\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm  # Fortschrittsbalken\n",
    "\n",
    "# === Audio-Handling ===\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# === Visualisierung ===\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# === Clustering ===\n",
    "from sklearn.cluster import DBSCAN, HDBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# import hdbscan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36270bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare Metrics\n",
    "ref = 'ich bin gross und stark' # original sentence\n",
    "hyp = 'bin ich so gross und stark' # hypothese to evaluate\n",
    "\n",
    "# WER and CER: (Substitutions+Insertions+Deletions)/N (Levenshtein Distance)\n",
    "wer_score = werpy.wers(ref, hyp)\n",
    "print(f'WER: {wer_score} (0 = best, 1 = same N but every word was replaced)')\n",
    "\n",
    "cer_score = cer(ref, hyp)\n",
    "print(f\"CER: {cer_score} (0 = best, 1 = same N but every char was replaced)\")\n",
    "\n",
    "# TP = words existing in ref and hyp, (TP+FP) = N_ref = number of words in ref, (TP+FN) = N_hyp = number of words in hyp\n",
    "# Precision: TP/(TP+FP) = TP/N_ref\n",
    "# Recall: TP/(TP+FN) =  TP/N_hyp\n",
    "# n-grams: sequence of n-words (e.g. 'ich bin', 'bin gross' ...)\n",
    "# BP = Brevity-Penalty (because a short hyp, having exactly the words in ref but not all leads into 100% BLEU)\n",
    "# if N_hyp > N_ref: BP = 1; if N_hyp <= N_ref: BP = e^(1-(N_ref/N_hyp))\n",
    "# BLEU --> penalizes short hyp\n",
    "# BLEU = BP*((1-gram Precision)^w1 * (2-gram Precision)^w2 * ...)\n",
    "# weights w: tells how much weight each n-gram has. idealy it sums up to 1\n",
    "# because sentences are \"only\" 6-12 words long, default BLEU of up to 4-gram is not usefull, 2-gram is used weights of 3-gram and 4-gram are set to 0 (0.5, 0.5, 0, 0)\n",
    "bleu_score_1gram = sentence_bleu([ref.split(' ')], hyp.split(' '), weights=(1, 0, 0, 0), smoothing_function=smooth)\n",
    "print(f\"bleu 1-gram: {bleu_score_1gram}\")\n",
    "\n",
    "bleu_score_2gram = sentence_bleu([ref.split(' ')], hyp.split(' '), weights=(0.5, 0.5, 0, 0), smoothing_function=smooth)\n",
    "print(f\"bleu 2-gram: {bleu_score_2gram}\")\n",
    "\n",
    "bleu_score_3gram = sentence_bleu([ref.split(' ')], hyp.split(' '), weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth)\n",
    "print(f\"bleu 3-gram: {bleu_score_3gram}\")\n",
    "\n",
    "bleu_score_4gram = sentence_bleu([ref.split(' ')], hyp.split(' '), weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)\n",
    "print(f\"bleu 4-gram: {bleu_score_4gram}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db59ccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = os.getcwd()\n",
    "INPUT_FILENAME = \"transcripted_train_all.tsv\"\n",
    "INPUT_PATH = os.path.join(BASE_PATH, \"datasets\", \"STT4SG-350\", \"Data_300\", \"Data_300\", \"train_all.tsv\")\n",
    "OUTPUT_PATH = os.path.join(BASE_PATH, \"transcripts\", \"transcripts_tsv\", \"tsv_350\", INPUT_FILENAME) \n",
    "JSON_PATH = os.path.join(BASE_PATH, \"transcripts\", \"transcripts_json\", \"json_350\")\n",
    "AUDIO_PATH = os.path.join(BASE_PATH, \"datasets\", \"STT4SG-350\", \"Data_300\", \"Data_300\", \"clips__train_valid\")\n",
    "\n",
    "# Thresholds to remove outliers and corrupt data\n",
    "WER_threshold = 0.94                      # keep < , zwischen 1 und 5 hat es sehr gute beispiele abre auch absolut nicht brauchbare, siehe unten\n",
    "CER_threshold = 1.0                      # keep < , zwischen 1 und 5 hat es sehr gute beispiele abre auch absolut nicht brauchbare, siehe unten\n",
    "BLEU1_threshold = 0.1                   # keep >\n",
    "BLEU2_threshold = 0.05                   # keep >\n",
    "transc_len_threshold_low = 11           # keep >\n",
    "transc_len_threshold_high = 160         # keep <\n",
    "transc_count_threshold_low = 2          # keep >, teilweise gute Sätze dabei, daher fraglich wie sinnvoll das zu filtern, bei 3...5 sind aber auch defekte audios dabei\n",
    "len_difference_threshold_low = -35      # keep > bis -37 sind es ausschliesslich unvollständige audio-clips, darüber dann gute samples mit CH satzstellungen aber auch noch vereinzelte schlechte clips\n",
    "len_difference_threshold_high = 31      # keep < , unter 31, gehen gute schweizerdeutsche Satzstellungen verloren\n",
    "len_diff_norm_threshold_low = -0.46      # keep >, Ausgeschriebene Zahlen und Wörter (z.B. prozent) in sentence vs. Ziffern (Abkürzungen z.b. % ) im Transcript, führen zu hohen negativen Werten\n",
    "len_diff_norm_threshold_high = 0.65      # keep <, hier umgekehrt, Ziffern in sentence, ausgeschriebene Zahlen in transcript\n",
    "\n",
    "audiosize_threshold = 38400             # in Bytes, keep > , \n",
    "duration_threshold = 1.9                # in seconds, keep > , Wert aus dem histogramm bestimmt, dort gibt es eine klare abgrenzung von kurzen clips\n",
    "confidence_threshold = 0.42            # keep >, confidence NaN werden weiter unten mit -1 ersetzt. das sind transcripts mit der Länge 0, diese können gefiltert werden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manuelle Qualitätsfilter\n",
    "Die Listen in diesem Abschnitt bündeln Nutzerinnen und Nutzer mit dauerhaft schlechter Audioqualität, typische Fehltranskripte und weitere Sonderfälle. Dadurch lässt sich jeder Filterentscheid vor dem eigentlichen Export transparent belegen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c0f215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# users where some files are from bad quality but not all, problem: approx 1100 clipes per user\n",
    "low_quality_users = [\n",
    "                    '1add488f-cc2b-41ad-b40d-ca96bfac5d26', \n",
    "                    '7b009b41-3f12-4757-944d-a12e4d33fba1',\n",
    "                    '08859fe6-2f51-4deb-ad9a-e4ad1aea56fa',\n",
    "                    '485dbe58-ff41-4008-a803-520e5244768a', \n",
    "                    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8bf903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some invalid clips create common transcripts, those transcripts are removed\n",
    "common_transcripts = [\n",
    "                        'so',                           # 1x short duration audioclip, bad quality \n",
    "                        'okay tschüss',                 # 1x short duration audioclip, bad quality  \n",
    "                        'tschüss',                      # 35x  crazy robosound vom user 1add488f-cc2b-41ad-b40d-ca96bfac5d26\n",
    "                        'vielen dank',                  # 34x bad audio quality leads into this common transcript\n",
    "                        'vielen dank fürs zuschauen',    # 12x bad audio quality, vallis, robosound\n",
    "                        'bis zum nächsten mal',         # 10x bad audio quality, vallis, robosound\n",
    "                        'vielen dank fürs zuschauen und bis zum nächsten mal',  # 9x bad audio quality, vallis\n",
    "                        'vielen dank für die aufmerksamkeit',                   # 6x bad audio quality, vallis\n",
    "                        'vielen dank für ihre aufmerksamkeit',                  # 6x bad audio quality, vallis\n",
    "                        'äh äh äh'                      # 5x robosound\n",
    "                        'das wars für heute wir sehen uns beim nächsten mal',                          # 4x bad audio quality, vallis\n",
    "                        'das wars für heute bis zum nächsten mal',                                     # 4x bad audio quality, vallis\n",
    "                        'untertitelung des zdf 2020',                                                  # 4x small audosize, bad qualityi\n",
    "                        'äh äh äh äh',                                                                 # 3x robosound\n",
    "                        'danke',                        # bad audio\n",
    "                        'amen',                         # short clips, bad quality, \n",
    "                        'ja',                           # noises, music\n",
    "                        'untertitelung br 2018',        # robo sound\n",
    "                        'das ist das wichtigste',       # short duration audioclip, bad quality, robosound\n",
    "                        'ciao',                         # short duration audioclip robosound\n",
    "                        'das wars für heute',           # bad audio quality  \n",
    "                        'das wars und wir sehen uns in der nächsten folge', \n",
    "                        '',\n",
    "                        ]\n",
    "\n",
    "# may some signs appear for defect audios\n",
    "special_signs = [\n",
    "                '*',\n",
    "                '#',\n",
    "                '(',\n",
    "                ')',\n",
    "                '@',\n",
    "                '\"',\n",
    "                '...',\n",
    "                ]                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53faf35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audios are clearly understandable, but the resulting transcript is a common sentence not fitting to the audios (different sentences)\n",
    "# nothing is filtered around this yet\n",
    "wrong_transcripts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ae8232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# following clips have a WER between 2...2.5 we want to keep swissgerman but not the others (lists are only a selection and not complete)\n",
    "# this data is not used yet anywhere\n",
    "\n",
    "# sentences which are spoken quite different in swiss german, but correct, may what we want, but those will be filtered \n",
    "# user 61b3a994-0787-4f02-915e-58740d1cd016 ist wahrscheinlich walliser, dieser Person hat die sätze teilweise stark verändert\n",
    "swissgerman = [] \n",
    "\n",
    "\n",
    "# clips listed here could not be filtered in any other way than WER, what also leads to filtering swissgerman clips\n",
    "# folgende Listen können bei Bedarf erweitert werden, diese werden dann aus dem df rausgenommen\n",
    "\n",
    "# some guys repeat, correct the sentence in one audio clip: \n",
    "correctors = []\n",
    "\n",
    "# some users do not like the sentence, so they say why they do not read that\n",
    "zu_kompliziert = []\n",
    "\n",
    "# audio is clear but not matching the sentence\n",
    "random_sentence = []     \n",
    "\n",
    "# long clips in bad quality\n",
    "long_bad_quality = []\n",
    "\n",
    "# concat lists to remove\n",
    "remove = correctors+zu_kompliziert+random_sentence+long_bad_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e45f011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clips(df_sorted, start=0, n_clips=5):\n",
    "    '''Function for plotting information about clip, including sentence, transcript and Playable Audio'''\n",
    "    end = start+n_clips\n",
    "    counter = -1\n",
    "\n",
    "    for i, row in df_sorted.iterrows():\n",
    "        if row['WER'] == 0:\n",
    "            continue\n",
    "\n",
    "        counter += 1\n",
    "        if counter < start:\n",
    "            continue\n",
    "        if counter >= end:\n",
    "            break\n",
    "\n",
    "        print(f'WER: {row['WER']:.3f} | CER {row['CER']:.3f} | BLEU 1-gram {row['BLEU_1gram']:.3f} |  BLEU 2-gram {row['BLEU_2gram']:.3f}  | '\n",
    "              f'confidence {row['confidence']:.3f} | transcript_len: {row['transcript_len']} | difference in length: {row['len_difference_norm']:.2f}% ({row['len_difference']} chars)')\n",
    "        print(f'sentence       {row['sentence']} | ({row['sentence_norm']})')\n",
    "        print(f'transcript     {row['transcript']} | ({row['transcript_norm']})\\n')\n",
    "        print(f'age: {row['age']} | gender: {row['gender']} | sentence_source: {row['sentence_source']} | dialect_region: {row['dialect_region']} | canton: {row['canton']}')\n",
    "        print(f'{row['audiosize']/(10**3):.3f} kB | {row['duration']:.2f} sec | clip path: {row['clip_path']}')\n",
    "        clip_path = os.path.join(AUDIO_PATH, row['clip_path'])\n",
    "        try:\n",
    "            data, samplerate = sf.read(clip_path)\n",
    "            display(Audio(data=data, rate=samplerate))\n",
    "        except Exception as e:\n",
    "            print(f\"[Fehler beim Abspielen: {e}]\")\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "def get_file_size(clip_path):\n",
    "    '''Function for loading Filesize, gives a hint on defect Audio Files'''\n",
    "    return os.path.getsize(os.path.join(AUDIO_PATH, clip_path))\n",
    "\n",
    "\n",
    "def is_in_string(target_string, certain_string_list):\n",
    "    '''function to filter string chunks being part of a string'''\n",
    "    for string in certain_string_list:\n",
    "        if string in target_string:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_certain_string(target_string, certain_string_list):\n",
    "    '''function to filter strings matching target exactly'''\n",
    "    for string in certain_string_list:\n",
    "        if string == target_string:\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cebab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.read_csv(OUTPUT_PATH, sep=\"\\t\")\n",
    "print(len(df_temp))\n",
    "df_temp.drop_duplicates(inplace=True)\n",
    "print(len(df_temp))\n",
    "df_temp.to_csv(OUTPUT_PATH, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d99d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all values from original dataset (INPUT_PATH) and dataset with transcribed sentences (OUTPUT_PATH)\n",
    "df = pd.read_csv(OUTPUT_PATH, sep=\"\\t\")\n",
    "\n",
    "df_input = pd.read_csv(INPUT_PATH, sep=\"\\t\")\n",
    "df_input.rename(columns={\"path\": \"clip_path\"}, inplace=True)\n",
    "\n",
    "df = df.merge(df_input[['clip_path', 'sentence_source', 'dialect_region',\n",
    "                        'canton', 'age', 'gender']],\n",
    "              on='clip_path', how='left')\n",
    "df.columns\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aad6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "# transcript has few NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383f71d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process NaN Values\n",
    "df['transcript']=df['transcript'].fillna(\"\")\n",
    "\n",
    "#numerical values, are set to -1 if values can not be negative\n",
    "df['confidence'] = df['confidence'].fillna(-1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19893dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f18992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Word error Rate WER for each sample (sentences are normalized first, removing punctuation, changing everything to lower case)\n",
    "# https://github.com/analyticsinmotion/werpy/blob/main/README.md \n",
    "df['sentence_norm'] = werpy.normalize(df['sentence'])\n",
    "df['transcript_norm'] = werpy.normalize(df['transcript'])\n",
    "df['WER'] = werpy.wers(df['sentence_norm'], df['transcript_norm'])\n",
    "print(f'WER Maximum: {df['WER'].max()}')\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd4f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate CER\n",
    "df['CER'] = df.apply(lambda row: cer(row['sentence_norm'], row['transcript_norm']), axis=1)\n",
    "print(f\"CER Maximum: {df['CER'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d67300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Bleu with 1-gram\n",
    "df['BLEU_1gram'] = df.apply(lambda row: sentence_bleu([row['sentence_norm'].split(' ')], row['transcript_norm'].split(' '), weights=(1, 0, 0, 0), smoothing_function=smooth), axis=1)\n",
    "print(f\"BLEU 1-gram Minimum: {df['BLEU_1gram'].min()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7768a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Bleu with 2-gram\n",
    "df['BLEU_2gram'] = df.apply(lambda row: sentence_bleu([row['sentence_norm'].split(' ')], row['transcript_norm'].split(' '), weights=(0.5, 0.5, 0, 0), smoothing_function=smooth), axis=1)\n",
    "print(f\"BLEU 2-gram Minimum: {df['BLEU_2gram'].min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8774f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Bleu with 2-gram\n",
    "df['BLEU_3gram'] = df.apply(lambda row: sentence_bleu([row['sentence_norm'].split(' ')], row['transcript_norm'].split(' '), weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth), axis=1)\n",
    "print(f\"BLEU 3-gram Minimum: {df['BLEU_3gram'].min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06de8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence_len'] = df['sentence_norm'].apply(len)\n",
    "df['transcript_len'] = df['transcript_norm'].apply(len)\n",
    "df['len_difference'] = df['transcript_len']-df['sentence_len']\n",
    "df['len_difference_norm'] = df['len_difference']/df['sentence_len']\n",
    "df['word_count_sent'] = df['sentence_norm'].apply(lambda x: len(str(x).split()))\n",
    "df['word_count_transc'] = df['transcript_norm'].apply(lambda x: len(str(x).split()))\n",
    "df['word_count_difference'] = df['word_count_transc']-df['word_count_sent']\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109a0665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract audiosize (hint for defect audiofiles)\n",
    "df['audiosize'] = df['clip_path'].apply(get_file_size)\n",
    "print(f\"min audiosize: {df['audiosize'].min()} Bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d378154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "audiosize_filter = (df['audiosize']>audiosize_threshold)\n",
    "print_clips(df[audiosize_filter].sort_values('audiosize', ascending=True), start=0, n_clips=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2301ac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 15))\n",
    "\n",
    "bins_audiosize = np.arange(0,100000, 100)\n",
    "plt.subplot(8, 1, 1, title='audio length in Bytes smallest files')\n",
    "plt.hist(df['audiosize'], bins=bins_audiosize)\n",
    "plt.grid()\n",
    "\n",
    "bins_audiosize = np.arange(0,400000, 1000)\n",
    "plt.subplot(8, 1, 2, title='audio length in Bytes')\n",
    "plt.hist(df['audiosize'], bins=bins_audiosize)\n",
    "plt.grid()\n",
    "\n",
    "bins_duration = np.arange(0,14, 0.1)\n",
    "plt.subplot(8, 1, 3, title='Audio-Clip Duration')\n",
    "plt.hist(df['duration'], bins=bins_duration)\n",
    "plt.grid()\n",
    "\n",
    "bins_len = np.arange(0,120, 1)\n",
    "plt.subplot(8, 1, 4, title=('sentence and transcript length'))\n",
    "plt.hist(df['sentence_len'], bins = bins_len, label=f'sentence length (min:{df['sentence_len'].min()}, max:{df['sentence_len'].max()})')\n",
    "plt.hist(df['transcript_len'], bins = bins_len, alpha=0.5, label=f'transcript length (min:{df['transcript_len'].min()}, max:{df['transcript_len'].max()})')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "bins_diff = np.arange(-0.5, 0.5, 0.01)\n",
    "plt.subplot(8, 1, 5, title=f'length difference in % to sentence  (min:{df['len_difference_norm'].min()}, max:{df['len_difference_norm'].max()})')\n",
    "plt.hist(df['len_difference_norm'], bins = bins_diff, alpha=0.5, label='length difference in % to sentence')\n",
    "plt.grid()\n",
    "\n",
    "bins_diff = np.arange(-40,40, 1)\n",
    "plt.subplot(8, 1, 6, title=f'length difference between transcript and sentence  (min:{df['len_difference'].min()}, max:{df['len_difference'].max()})')\n",
    "plt.hist(df['len_difference'], bins = bins_diff, alpha=0.5, label='length difference')\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "bins_words = np.arange(0,20, 1)\n",
    "plt.subplot(8, 1, 7, title=('sentence and transcript length'))\n",
    "plt.hist(df['word_count_sent'], bins = bins_words, label=f'sentence word count (min:{df['word_count_sent'].min()}, max:{df['word_count_sent'].max()})')\n",
    "plt.hist(df['word_count_transc'], bins = bins_words, alpha=0.5, label=f'transcript word count (min:{df['word_count_transc'].min()}, max:{df['word_count_transc'].max()})')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "bins_words_diff = np.arange(-20,20, 1)\n",
    "plt.subplot(8, 1, 8, title=f'word count difference between transcript and sentence  (min:{df['word_count_difference'].min()}, max:{df['word_count_difference'].max()})')\n",
    "plt.hist(df['len_difference'], bins = bins_words_diff, alpha=0.5, label='length difference')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9815dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics filters\n",
    "WER_filter = (df['WER']<WER_threshold)  \n",
    "CER_filter = (df['CER']<CER_threshold) \n",
    "BLEU1_filter = (df['BLEU_1gram']>BLEU1_threshold)  \n",
    "BLEU2_filter = (df['BLEU_2gram']>BLEU2_threshold)\n",
    "confidence_filter = (df['confidence']>confidence_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4334eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analize WER, CER, BLEU, confidence\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "bins_er = np.arange(0,2, 0.02)\n",
    "plt.subplot(3, 1, 1, title='WER / CER')\n",
    "plt.hist(df['WER'], bins=bins_er, label='WER')\n",
    "plt.hist(df['CER'], bins=bins_er, alpha=0.5, label='CER')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "bins_bleu = np.arange(0,1, 0.01)\n",
    "plt.subplot(3, 1, 2, title='BLEU')\n",
    "plt.hist(df['BLEU_1gram'], bins=bins_bleu, label='BLEU 1-gram')\n",
    "plt.hist(df['BLEU_2gram'], bins=bins_bleu, alpha=0.5, label='BLEU 2-gram')\n",
    "plt.hist(df['BLEU_3gram'], bins=bins_bleu, alpha=0.3, label='BLEU 3-gram')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "bins_confidence = np.arange(0,1, 0.02)\n",
    "plt.subplot(3, 1, 3, title='confidence score')\n",
    "plt.hist(df['confidence'], bins=bins_confidence)\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8e0b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CER_filter'] = CER_filter\n",
    "df['confidence_filter'] = confidence_filter\n",
    "sns.pairplot(df[df['audiosize'] > 10000], vars=['WER', 'CER', 'confidence'], hue = 'CER_filter')\n",
    "plt.legend(title=f'CER < {CER_threshold}')\n",
    "\n",
    "df.drop(['CER_filter', 'confidence_filter'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47afc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BLEU2_filter'] = BLEU2_filter\n",
    "sns.pairplot(df[df['audiosize'] > 10000], vars=['WER', 'CER', 'BLEU_1gram'], hue = 'BLEU2_filter')\n",
    "plt.legend(title=f'BLEU2_filter < {BLEU2_threshold}')\n",
    "\n",
    "df.drop(['BLEU2_filter'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd633a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze WER in Detail\n",
    "print(f'transcirpts with no differences (WER=0): {len(df[df['WER']==0])} / {len(df)} = {len(df[df['WER']==0])/len(df)*100:.2f} %')\n",
    "print(f'transcirpts with marginal differences (0<WER<0.25): {len(df[(0<df['WER'])&(df['WER']<0.25)])} / {len(df)} = {len(df[df['WER']==0])/len(df)*100:.2f} %')\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.suptitle('WER distribution')\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.boxplot(df['WER'], orientation='horizontal')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.boxplot(df['WER'], orientation='horizontal')\n",
    "plt.xlim(0, 5)\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.boxplot(df['WER'], orientation='horizontal')\n",
    "plt.xlim(0, 1)\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# WER above 4 can be cut, easely, above 2 probably also, and may even above 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e894f1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze CER in Detail\n",
    "print(f'transcirpts with no differences (CER=0): {len(df[df['CER']==0])} / {len(df)} = {len(df[df['CER']==0])/len(df)*100:.2f} %')\n",
    "print(f'transcirpts with marginal differences (0<CER<0.25): {len(df[(0<df['CER'])&(df['CER']<0.25)])} / {len(df)} = {len(df[df['CER']==0])/len(df)*100:.2f} %')\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.suptitle('CER distribution')\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.boxplot(df['CER'], orientation='horizontal')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.boxplot(df['CER'], orientation='horizontal')\n",
    "plt.xlim(0, 2.5)\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.boxplot(df['CER'], orientation='horizontal')\n",
    "plt.xlim(0, 1)\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62460ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analize CER depending on other columns\n",
    "bins = np.arange(0, 1.2, 0.1)\n",
    "labels = [f\"{i:.1f}-{i+0.1:.1f}\" for i in bins[:-1]]\n",
    "\n",
    "# Liste der Spalten\n",
    "columns = ['sentence_source', 'age', 'dialect_region', 'canton', 'gender', 'confidence']\n",
    "\n",
    "# Subplots erstellen (3 Zeilen, 2 Spalten)\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "fig.suptitle('CER Analyse', fontsize=16)\n",
    "\n",
    "# **Kopie von `df` erstellen**\n",
    "df_copy = df.copy(deep=True)\n",
    "\n",
    "# Daten in Bins einteilen (nur in der Kopie)\n",
    "df_copy['bin'] = pd.cut(df_copy['CER'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Schleife über die Spalten\n",
    "for i, col in enumerate(columns):\n",
    "    ax = axes[i // 2, i % 2]\n",
    "\n",
    "    if col == 'confidence':\n",
    "        # Float-Spalte in Kategorien umwandeln (nur in der Kopie)\n",
    "        float_bins = [0, 0.25, 0.5, 0.75, 1.0]\n",
    "        float_labels = ['0–0.25', '0.25–0.5', '0.5–0.75', '0.75–1.0']\n",
    "        df_copy[col] = pd.cut(\n",
    "            df_copy[col].astype(float),\n",
    "            bins=float_bins,\n",
    "            labels=float_labels,\n",
    "            right=False\n",
    "        ).astype(str)\n",
    "        df_copy[col] = df_copy[col].fillna('NaN')\n",
    "    else:\n",
    "        # Andere Spalten: NaN als String umwandeln (nur in der Kopie)\n",
    "        df_copy[col] = df_copy[col].fillna('NaN').astype(str)\n",
    "\n",
    "    # Anzahl der Werte pro Kategorie berechnen (basierend auf der Kopie)\n",
    "    value_counts = df_copy[col].value_counts(dropna=False).to_dict()\n",
    "\n",
    "    # Prozentuale Anteile pro Kategorie und Bin berechnen\n",
    "    result = (\n",
    "        df_copy.groupby(['bin', col], observed=False)\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "        .apply(lambda x: x / x.sum() * 100, axis=0)\n",
    "    )\n",
    "\n",
    "    # Plotten\n",
    "    result.plot(kind='bar', stacked=False, width=0.8, ax=ax)\n",
    "\n",
    "    # Legenden-Einträge mit Anzahl der Werte ergänzen\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    new_labels = [f\"{label} ({value_counts.get(label, 0)})\" for label in labels]\n",
    "    ax.legend(handles, new_labels, title=col)\n",
    "\n",
    "    ax.set_title(f'CER for {col}')\n",
    "    ax.set_xlabel('CER')\n",
    "    ax.set_ylabel('percentage of own category [%]')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0618d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analize BLEU depending on other columns\n",
    "bins = np.arange(0, 1, 0.1)\n",
    "labels = [f\"{i:.1f}-{i+0.1:.1f}\" for i in bins[:-1]]\n",
    "\n",
    "# Liste der Spalten\n",
    "columns = ['sentence_source', 'age', 'dialect_region', 'canton', 'gender', 'confidence']\n",
    "\n",
    "# Subplots erstellen (3 Zeilen, 2 Spalten)\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "fig.suptitle('BLEU Analyse', fontsize=16)\n",
    "\n",
    "# **Kopie von `df` erstellen**\n",
    "df_copy = df.copy(deep=True)\n",
    "\n",
    "# Daten in Bins einteilen (nur in der Kopie)\n",
    "df_copy['bin'] = pd.cut(df_copy['BLEU_2gram'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Schleife über die Spalten\n",
    "for i, col in enumerate(columns):\n",
    "    ax = axes[i // 2, i % 2]\n",
    "\n",
    "    if col == 'confidence':\n",
    "        # Float-Spalte in Kategorien umwandeln (nur in der Kopie)\n",
    "        float_bins = [0, 0.25, 0.5, 0.75, 1.0]\n",
    "        float_labels = ['0–0.25', '0.25–0.5', '0.5–0.75', '0.75–1.0']\n",
    "        df_copy[col] = pd.cut(\n",
    "            df_copy[col].astype(float),\n",
    "            bins=float_bins,\n",
    "            labels=float_labels,\n",
    "            right=False\n",
    "        ).astype(str)\n",
    "        df_copy[col] = df_copy[col].fillna('NaN')\n",
    "    else:\n",
    "        # Andere Spalten: NaN als String umwandeln (nur in der Kopie)\n",
    "        df_copy[col] = df_copy[col].fillna('NaN').astype(str)\n",
    "\n",
    "    # Anzahl der Werte pro Kategorie berechnen (basierend auf der Kopie)\n",
    "    value_counts = df_copy[col].value_counts(dropna=False).to_dict()\n",
    "\n",
    "    # Prozentuale Anteile pro Kategorie und Bin berechnen\n",
    "    result = (\n",
    "        df_copy.groupby(['bin', col], observed=False)\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "        .apply(lambda x: x / x.sum() * 100, axis=0)\n",
    "    )\n",
    "\n",
    "    # Plotten\n",
    "    result.plot(kind='bar', stacked=False, width=0.8, ax=ax)\n",
    "\n",
    "    # Legenden-Einträge mit Anzahl der Werte ergänzen\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    new_labels = [f\"{label} ({value_counts.get(label, 0)})\" for label in labels]\n",
    "    ax.legend(handles, new_labels, title=col)\n",
    "\n",
    "    ax.set_title(f'BLEU 2-gram for {col}')\n",
    "    ax.set_xlabel('BLEU 2-gram')\n",
    "    ax.set_ylabel('percentage of own category [%]')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea9f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for special signs\n",
    "special_signs_filter = (df['transcript'].apply(is_in_string, certain_string_list=special_signs))\n",
    "\n",
    "print(f'clips including special signs {special_signs}: {len(df[(special_signs_filter)])}')\n",
    "print()\n",
    "\n",
    "print_clips(df[df['transcript_len']>0][(special_signs_filter)].sort_values('CER', ascending=False), start=0, n_clips=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756935e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze users having several bad audio clips\n",
    "low_quality_user_filter = (df['clip_path'].apply(is_in_string, certain_string_list=low_quality_users))\n",
    "\n",
    "print(f'amount of audio clips from low quality users: {len(df[(low_quality_user_filter)])}')\n",
    "print()\n",
    "\n",
    "print_clips(df[df['transcript_len']>0][(low_quality_user_filter)].sort_values('BLEU_1gram', ascending=False), start=0, n_clips=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be70d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze Audio clip qualities depending on age\n",
    "plt.figure(figsize=(15,4))\n",
    "\n",
    "# Kategorien für confidence erstellen\n",
    "bins = [-2, 0, confidence_threshold, float('inf')]\n",
    "labels = [\"NA\", f\"bad <={confidence_threshold}\", f\"good >{confidence_threshold}\"]\n",
    "df['confidence_cat'] = pd.cut(\n",
    "    df['confidence'],\n",
    "    bins=bins,\n",
    "    labels=labels,\n",
    "    right=True\n",
    ")\n",
    "\n",
    "\n",
    "# Funktion um auch absolute Counts in der crosstab anzuzeigen\n",
    "def get_annotations(crosstab, tab_count):\n",
    "    annot = np.empty_like(crosstab.values, dtype=object)\n",
    "    for i in range(crosstab.shape[0]):\n",
    "        for j in range(crosstab.shape[1]):\n",
    "            value = crosstab.iloc[i, j]\n",
    "            count = tab_count.iloc[i, j] if j < tab_count.shape[1] else 0\n",
    "            annot[i, j] = f\"{value:.2f}\\n({int(count)})\"\n",
    "    return annot\n",
    "\n",
    "# 1. Plot: CER < threshold\n",
    "plt.subplot(1, 3, 1, title=f'Percentage of CER < {CER_threshold}')\n",
    "tab = pd.crosstab(df['age'], (df['CER'] < CER_threshold), normalize='index')\n",
    "tab_count = pd.crosstab(df['age'], (df['CER'] < CER_threshold))\n",
    "annot = get_annotations(tab, tab_count)\n",
    "sns.heatmap(tab, cmap=sns.cubehelix_palette(as_cmap=True),\n",
    "            annot=annot, fmt='', linewidths=0.5)\n",
    "\n",
    "\n",
    "# 3. Plot: confidence (drei Kategorien)\n",
    "plt.subplot(1, 3, 2, title=f'confidence')\n",
    "tab = pd.crosstab(df['age'], df['confidence_cat'], normalize='index')\n",
    "tab_count = pd.crosstab(df['age'], df['confidence_cat'])\n",
    "annot = get_annotations(tab, tab_count)\n",
    "sns.heatmap(tab, cmap=sns.cubehelix_palette(as_cmap=True),\n",
    "            annot=annot, fmt='', linewidths=0.5)\n",
    "\n",
    "# 1. Plot: audiosize < threshold\n",
    "plt.subplot(1, 3, 3, title=f'Percentage of audiosize < {audiosize_threshold}')\n",
    "tab = pd.crosstab(df['age'], (df['audiosize'] < audiosize_threshold), normalize='index')\n",
    "tab_count = pd.crosstab(df['age'], (df['audiosize'] < audiosize_threshold))\n",
    "annot = get_annotations(tab, tab_count)\n",
    "sns.heatmap(tab, cmap=sns.cubehelix_palette(as_cmap=True),\n",
    "            annot=annot, fmt='', linewidths=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c925ae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze common transcripts\n",
    "# Zähle die Häufigkeit jedes unique Werts in 'transcript_norm'\n",
    "value_counts = df[df['duration']>duration_threshold]['transcript_norm'].value_counts()\n",
    "\n",
    "# Filtere nur Werte mit mehr als 5 Vorkommen, Länge > Schwellwert und nicht in 'sentence_norm'\n",
    "frequent_long_unique_values = value_counts[\n",
    "    (value_counts > 2) &\n",
    "    (~value_counts.index.isin(df['sentence_norm'].unique()))\n",
    "]\n",
    "\n",
    "# Ausgabe\n",
    "print(f\"Unique Werte in 'transcript_norm' (Häufigkeit > 5, duration > {duration_threshold}, nicht in 'sentence_norm'):\")\n",
    "print(frequent_long_unique_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6700669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_transcript_filter = (df['transcript_norm'].apply(is_certain_string, certain_string_list=common_transcripts))\n",
    "\n",
    "print(f'amount of transcripts having common sentences: {len(df[(common_transcript_filter)])},'\n",
    "      f'(duration > {duration_threshold}: {len(df[(common_transcript_filter) & (df['duration']>duration_threshold)])})')\n",
    "print()\n",
    "\n",
    "print_clips(df[(common_transcript_filter) & (df['duration']>duration_threshold)].sort_values('CER', ascending=True), start=0, n_clips=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairplot_columns = ['WER', 'CER', 'sentence_len', 'transcript_len', 'len_difference', 'audiosize', 'duration', 'confidence', 'dialect_region']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e02610",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[df['audiosize'] > audiosize_threshold][pairplot_columns], hue='dialect_region')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4207d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many samples are filtered by threshold\n",
    "# WER_filter, CER_filter, audiosize_filter and confidence_filter are already defined earlier           \n",
    "trlen_low_filter     = (df['transcript_len'] >transc_len_threshold_low     )\n",
    "trlen_high_filter    = (df['transcript_len'] <transc_len_threshold_high    )\n",
    "len_diff_low_filter  = (df['len_difference'] >len_difference_threshold_low )\n",
    "len_diff_high_filter = (df['len_difference'] <len_difference_threshold_high)\n",
    "len_diff_norm_low_filter  = (df['len_difference_norm'] >len_diff_norm_threshold_low )\n",
    "len_diff_norm_high_filter = (df['len_difference_norm'] <len_diff_norm_threshold_high)\n",
    "trcount_low_filter   = (df['word_count_transc'] >transc_count_threshold_low)\n",
    "duration_filter      = (df['duration']       >duration_threshold           )\n",
    "\n",
    "\n",
    "# filter data based on findings\n",
    "df_filter = df[ (WER_filter          )\n",
    "              & (CER_filter          )\n",
    "              & (BLEU1_filter        )\n",
    "              & (BLEU2_filter        )                            \n",
    "              & (confidence_filter   )             \n",
    "              & (trlen_low_filter    )\n",
    "              & (trlen_high_filter   )\n",
    "              & (len_diff_low_filter )\n",
    "              & (len_diff_high_filter)\n",
    "              & (len_diff_norm_low_filter )\n",
    "              & (len_diff_norm_high_filter)\n",
    "              & (trcount_low_filter  )             \n",
    "              & (audiosize_filter    )\n",
    "              & (duration_filter     )\n",
    "              & (duration_filter     )\n",
    "              #& (~low_quality_user_filter ) from 1100 clips per user, its may not a good idea to remove complete user\n",
    "              & (~common_transcript_filter)\n",
    "              & (~special_signs_filter)\n",
    "              ]\n",
    "\n",
    "filtered_clips = len(df_filter)\n",
    "removed_clips = len(df) - filtered_clips\n",
    "\n",
    "print(f'WER Threshold: {WER_threshold}, will remove {len(df[~WER_filter])} samples')\n",
    "print(f'CER Threshold: {CER_threshold}, will remove {len(df[~CER_filter])} samples')\n",
    "print(f'BLEU 1-gram Threshold: {BLEU1_threshold}, will remove {len(df[~BLEU1_filter])} samples')\n",
    "print(f'BLEU 2-gram Threshold: {BLEU2_threshold}, will remove {len(df[~BLEU2_filter])} samples')\n",
    "print(f'confidence Threshold: {confidence_threshold}, will remove {len(df[~confidence_filter])} samples')\n",
    "print()\n",
    "print(f'Transcript length Low Threshold: {transc_len_threshold_low}, will remove {len(df[~trlen_low_filter])} samples')\n",
    "print(f'Transcript length High Threshold: {transc_len_threshold_high}, will remove {len(df[~trlen_high_filter])} samples')\n",
    "print()\n",
    "print(f'len difference Low Threshold: {len_difference_threshold_low}, will remove {len(df[~len_diff_low_filter])} samples')\n",
    "print(f'len difference High Threshold: {len_difference_threshold_high}, will remove {len(df[~len_diff_high_filter])} samples')\n",
    "print(f'len difference Norm Low Threshold: { len_diff_norm_threshold_low}, will remove { len(df[~len_diff_norm_low_filter])} samples')\n",
    "print(f'len difference Norm High Threshold: {len_diff_norm_threshold_high}, will remove {len(df[~len_diff_norm_high_filter])} samples')\n",
    "print()\n",
    "print(f'Transcript word count Low Threshold: {transc_count_threshold_low}, will remove {len(df[~trcount_low_filter])} samples')\n",
    "print()\n",
    "print(f'Audiosize Threshold: {audiosize_threshold} Bytes, will remove {len(df[~audiosize_filter])} samples')\n",
    "print(f'Duration Threshold: {duration_threshold} sec, will remove {len(df[~duration_filter])} samples')\n",
    "print()\n",
    "print(f'Low Quality users filter will remove {len(df[low_quality_user_filter])} samples')\n",
    "print()\n",
    "print(f'Common transcript filter will remove {len(df[common_transcript_filter])} samples')\n",
    "print(f'Special signs filter will remove {len(df[special_signs_filter])} samples')\n",
    "\n",
    "\n",
    "print(f'\\n\\nfiltered df count: {filtered_clips}, removed clips count: {removed_clips}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f4c0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# highest WERs in filtered data\n",
    "print(f'WER_threshold: {WER_threshold}\\n***********************************************')\n",
    "print_clips(df_filter.sort_values('WER', ascending=False), start=0, n_clips=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eea877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowest confidence in filtered data\n",
    "print(f'confidence_threshold: {confidence_threshold}\\n***********************************************')\n",
    "print_clips(df_filter.sort_values('confidence', ascending=True), start=0, n_clips=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37907c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowest BLEUs in filtered data\n",
    "print(f'BLEU1_threshold: {BLEU1_threshold}\\n***********************************************')\n",
    "print_clips(df_filter.sort_values('BLEU_1gram', ascending=True), start=0, n_clips=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5976953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortest transcripts in filtered data\n",
    "print(f'transc_len_threshold_low: {transc_len_threshold_low}\\n***********************************************')\n",
    "print_clips(df_filter.sort_values('transcript_len', ascending=True), start=0, n_clips=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8873efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# longest transcripts in filtered data\n",
    "print(f'transc_len_threshold_high: {transc_len_threshold_high}\\n***********************************************')\n",
    "print_clips(df_filter.sort_values('transcript_len', ascending=False), start=0, n_clips=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b486b857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcript way shorter than sentence\n",
    "print(f'len_difference_threshold_low: {len_difference_threshold_low}\\n***********************************************')\n",
    "print_clips(df_filter.sort_values('len_difference', ascending = True), start=0, n_clips=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0a4418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcript way longer than sentence\n",
    "print(f'len_difference_threshold_high: {len_difference_threshold_high}\\n***********************************************')\n",
    "print_clips(df_filter.sort_values('len_difference', ascending = False), start=0, n_clips=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7804f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcript way shorter than sentence\n",
    "print(f'len_diff_norm_threshold_low: {len_diff_norm_threshold_low}\\n***********************************************')\n",
    "print_clips(df_filter.sort_values('len_difference_norm', ascending = True), start=0, n_clips=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de511d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcript way longer than sentence\n",
    "print(f'len_diff_norm_threshold_high: {len_diff_norm_threshold_high}\\n***********************************************')\n",
    "print_clips(df_filter.sort_values('len_difference_norm', ascending = False), start=0, n_clips=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d300b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# short word count transcripts\n",
    "print(f'transc_count_threshold_low (word count): {transc_count_threshold_low}\\n***********************************************')\n",
    "print_clips(df_filter.sort_values('word_count_transc', ascending = True), start=0, n_clips=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eccd93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered dataset to tsv\n",
    "df_filter.to_csv(f'filtered_{INPUT_FILENAME}', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e36dd68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".qa_pip_lenovo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}